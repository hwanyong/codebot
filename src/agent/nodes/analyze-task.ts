import { RunnableConfig } from '@langchain/core/runnables';
import { HumanMessage, AIMessage } from '@langchain/core/messages';
import { State, Update } from '../state.js';
import { TaskAnalysis } from '../../types/index.js';
import { TASK_ANALYSIS_PROMPT } from '../../prompts/index.js';
import { Logger } from '../../utils/logger.js';

/**
 * Task analysis node
 * 태스크 분석 노드
 * Analyzes user request and classifies it into subtasks
 * 사용자 요청을 분석하고 하위 작업으로 분류합니다
 * @param state Current state (현재 상태)
 * @returns State update (상태 업데이트)
 *
 * Generated by Copilot
 */
export async function nodeAnalyzeTask(state: State): Promise<Update> {
  Logger.nodeEntry('analyzeTask');

  // Get the last user message
  // 마지막 사용자 메시지 가져오기
  const lastMessage = state.messages[state.messages.length - 1];

  // Create task analysis prompt using direct approach to avoid template issues
  // 템플릿 문제를 피하기 위해 직접 접근 방식을 사용하여 작업 분석 프롬프트 생성
  Logger.nodeAction('analyzeTask', 'Creating task analysis prompt');

  // Format the prompt directly to avoid template parsing issues with JSON braces
  // JSON 중괄호로 인한 템플릿 파싱 문제를 피하기 위해 프롬프트를 직접 포맷팅
  const formattedPrompt = TASK_ANALYSIS_PROMPT.replace('{user_request}', lastMessage.content as string);
  const promptValue = [new HumanMessage(formattedPrompt)];

  // Model call configuration
  // 모델 호출 설정
  const config: RunnableConfig = {
    configurable: {
      model: state.context.model
    }
  };

  try {
    // Call model with streaming
    // 스트리밍으로 모델 호출
    Logger.nodeAction('analyzeTask', 'Calling model for task analysis');
    Logger.nodeModelStart('analyzeTask', 'Starting model streaming for task analysis');

    // Stream response using the model's streaming capability
    // 모델의 스트리밍 기능을 사용하여 응답 스트리밍
    const stream = await state.context.model.stream(promptValue, config);

    // Collect the full response while streaming individual tokens
    // 개별 토큰을 스트리밍하면서 전체 응답 수집
    let resultContent = '';
    for await (const chunk of stream) {
      const content = chunk.content;
      if (content) {
        // 모델 스트리밍 이벤트 발생
        Logger.nodeModelStreaming('analyzeTask', content);
        resultContent += content;
      }
    }

    // 모델 응답 완료 이벤트 발생
    Logger.nodeModelEnd('analyzeTask');

    // Parse JSON response
    // JSON 응답 파싱
    Logger.nodeAction('analyzeTask', 'Parsing task analysis result');
    let taskAnalysis: TaskAnalysis;
    try {
      const jsonMatch = resultContent.match(/```json\n([\s\S]*?)\n```/) || resultContent.match(/({[\s\S]*})/);
      if (jsonMatch) {
        taskAnalysis = JSON.parse(jsonMatch[1]);
      } else {
        taskAnalysis = JSON.parse(resultContent);
      }
      Logger.nodeAction('analyzeTask', `Task analysis complete with ${taskAnalysis.subtasks?.length || 0} subtasks`);
      Logger.graphState('Task Analysis', taskAnalysis);
    } catch (error) {
      Logger.error('Failed to parse task analysis', error);
      Logger.nodeExit('analyzeTask', 'error');
      return {
        context: {
          ...state.context,
          lastError: {
            message: 'Unable to parse task analysis result.',
            timestamp: new Date().toISOString(),
            type: 'ParseError',
            stack: error instanceof Error ? error.stack : undefined
          },
          executionStatus: 'error'
        } as any
      };
    }

    // Save analysis result
    // 분석 결과 저장
    const aiMessage = new AIMessage(resultContent);

    Logger.nodeExit('analyzeTask');
    return {
      messages: [aiMessage],
      context: {
        currentTask: taskAnalysis,
        executionStatus: 'running'
      } as any
    };
  } catch (error) {
    Logger.error('Error in task analysis', error);
    Logger.nodeExit('analyzeTask', 'error');
    throw error;
  }
}