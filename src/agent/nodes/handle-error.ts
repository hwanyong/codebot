import { ChatPromptTemplate } from '@langchain/core/prompts';
import { RunnableConfig } from '@langchain/core/runnables';
import { AIMessage } from '@langchain/core/messages';
import { State, Update } from '../state.js';
import { HANDLE_ERROR_PROMPT } from '../../prompts/index.js';
import { Logger } from '../../utils/logger.js';

/**
 * Error handling node
 * 오류 처리 노드
 * Processes occurred errors
 * 발생한 오류를 처리합니다
 * @param state Current state (현재 상태)
 * @returns State update (상태 업데이트)
 *
 * Generated by Copilot
 */
export async function nodeHandleError(state: State): Promise<Update> {
  Logger.nodeEntry('handleError');

  // Get error information
  // 오류 정보 가져오기
  const { lastError } = state.context;

  if (!lastError) {
    Logger.error('No error information available');
    Logger.nodeExit('handleError');

    return {
      context: {
        ...state.context,
        executionStatus: 'error'
      } as any
    };
  }

  Logger.graphState('Error Information', lastError);

  // Create error handling prompt
  // 오류 처리 프롬프트 생성
  Logger.nodeAction('handleError', 'Creating error handling prompt');
  const handleErrorPrompt = ChatPromptTemplate.fromTemplate(HANDLE_ERROR_PROMPT);

  // Render prompt
  // 프롬프트 렌더링
  const promptValue = await handleErrorPrompt.formatMessages({
    error_info: JSON.stringify(lastError, null, 2),
    context: JSON.stringify({
      currentTask: state.context.currentTask,
      currentStepIndex: state.context.currentStepIndex,
      totalSteps: state.context.totalSteps
    }, null, 2)
  });

  // Model call configuration
  // 모델 호출 설정
  const config: RunnableConfig = {
    configurable: {
      model: state.context.model
    }
  };

  try {
    // Call model with streaming
    // 스트리밍으로 모델 호출
    Logger.nodeAction('handleError', 'Calling model for error handling');
    Logger.nodeModelStart('handleError', 'Starting model streaming for error handling');

    // Stream response using the model's streaming capability
    // 모델의 스트리밍 기능을 사용하여 응답 스트리밍
    const stream = await state.context.model.stream(promptValue, config);

    // Collect the full response while streaming individual tokens
    // 개별 토큰을 스트리밍하면서 전체 응답 수집
    let resultContent = '';
    for await (const chunk of stream) {
      const content = chunk.content;
      if (content) {
        // 모델 스트리밍 이벤트 발생
        Logger.nodeModelStreaming('handleError', content);
        resultContent += content;
      }
    }

    // 모델 응답 완료 이벤트 발생
    Logger.nodeModelEnd('handleError');

    // Parse error handling result
    // 오류 처리 결과 파싱
    Logger.nodeAction('handleError', 'Parsing error handling response');
    let errorHandling;
    try {
      const content = resultContent;
      const jsonMatch = content.match(/```json\n([\s\S]*?)\n```/) || content.match(/({[\s\S]*})/);
      if (jsonMatch) {
        errorHandling = JSON.parse(jsonMatch[1]);
      } else {
        errorHandling = JSON.parse(content);
      }
      Logger.graphState('Error Handling Result', errorHandling);
    } catch (error) {
      // Use original response if parsing fails
      // 파싱 오류 시 원본 응답 사용
      Logger.error('Failed to parse error handling result', error);
      errorHandling = {
        error_type: lastError.type,
        cause: lastError.message,
        resolution: 'Unable to resolve error.',
        user_message: resultContent
      };
      Logger.graphState('Error Handling Fallback', errorHandling);
    }

    // Return error handling result
    // 오류 처리 결과 반환
    Logger.nodeAction('handleError', 'Error handling complete');
    Logger.nodeExit('handleError');

    return {
      messages: [new AIMessage(errorHandling.user_message)],
      context: {
        ...state.context,
        errorHandling,
        executionStatus: 'error'
      } as any
    };
  } catch (error) {
    Logger.error('Error in error handling', error);
    Logger.nodeExit('handleError', 'error');
    throw error;
  }
}